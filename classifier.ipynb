{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMdiQqwZqxmLadQ5EC1EaQP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varshil009/FaceInpainting/blob/main/classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDQKZERaZE9G",
        "outputId": "c689aa10-6cb9-4357-be9d-b30fe28272b0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## THE GOAL IS MAKE A CLASSIFICATION MODEL THAT CAN IDENTIFY FACES WHEN THEY ARE OCCLUDED, BUT BEFORE THAT THE MODEL SHOULD HAVE INFORMATION ABOUT FACES"
      ],
      "metadata": {
        "id": "yBk6hNsGZRuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "LtT7mWcSZbLk"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model 1"
      ],
      "metadata": {
        "id": "xdiLmwATF3KU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class classfier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.convolution = nn.Sequential(\n",
        "            # image shape = 218, 178, 3\n",
        "            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1, dilation=2),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(8),  # Replacing LayerNorm\n",
        "\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1, dilation=2),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, dilation=2),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, dilation=2),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, dilation=2),\n",
        "            nn.AdaptiveAvgPool2d((4, 3)),  # Fixed output size\n",
        "            nn.BatchNorm2d(128)  # Normalizing before fully connected layers\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 4 * 3, 512),  # Correct input size: 1536\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(256, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "\n",
        "            nn.Linear(64, 10)  # Assuming 10 classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convolution(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "3VXpCRVxZiku"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model 2"
      ],
      "metadata": {
        "id": "bk1NvPcWF497"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FaceRecognizer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FaceRecognizer, self).__init__()\n",
        "        self.convolution = models.efficientnet_b0(pretrained=True)\n",
        "        self.convolution.fc = nn.Identity() # removes fully connected dense layer\n",
        "        self.classifier = nn.Sequential(\n",
        "\n",
        "            nn.Linear(1000, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convolution(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "-8xoZobKF6rC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## make a dataloder that creates batches for data processing"
      ],
      "metadata": {
        "id": "8SXA9Th0jyJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_loaders_not_used(\n",
        "    data_dir,\n",
        "    batch_size=10,\n",
        "    image_size=(218, 178),\n",
        "    test_split=0.2\n",
        "):\n",
        "    \"\"\"\n",
        "    Create data loaders for numeric class folders\n",
        "\n",
        "    Expected folder structure:\n",
        "    data_dir/\n",
        "    ├── 0/\n",
        "    │   ├── anyimagename1.jpg\n",
        "    │   ├── anyimagename2.jpg\n",
        "    ├── 1/\n",
        "    │   ├── anyimagename1.jpg\n",
        "    │   ├── anyimagename2.jpg\n",
        "    \"\"\"\n",
        "\n",
        "    # Transformations\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    # Load entire dataset\n",
        "    full_dataset = datasets.ImageFolder(\n",
        "        root=data_dir,\n",
        "        transform=data_transforms\n",
        "    )\n",
        "\n",
        "    # Get total number of samples\n",
        "    dataset_size = len(full_dataset)\n",
        "\n",
        "    # Calculate validation size\n",
        "    val_size = int(dataset_size * test_split)\n",
        "    train_size = dataset_size - val_size\n",
        "\n",
        "    # Split dataset\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        full_dataset,\n",
        "        [train_size, val_size]\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=1\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    # Get class names (numeric in this case)\n",
        "    class_names = [int(s) for s in ['2820', '3227', '3699', '3745', '3782', '4887', '6568', '8968', '9152', '9256']]\n",
        "\n",
        "    return train_loader, val_loader, class_names\n",
        "\n",
        "def create_data_loaders(\n",
        "    data_dir,\n",
        "    batch_size=10,\n",
        "    image_size=(218, 178),\n",
        "    test_split=0.2\n",
        "):\n",
        "    \"\"\"\n",
        "    Create data loaders for numeric class folders.\n",
        "\n",
        "    Expected folder structure:\n",
        "    data_dir/\n",
        "    ├── 2820/\n",
        "    │   ├── anyimagename1.jpg\n",
        "    │   ├── anyimagename2.jpg\n",
        "    ├── 3227/\n",
        "    │   ├── anyimagename1.jpg\n",
        "    │   ├── anyimagename2.jpg\n",
        "    \"\"\"\n",
        "\n",
        "    # Transformations\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    # Load entire dataset\n",
        "    full_dataset = datasets.ImageFolder(\n",
        "        root=data_dir,\n",
        "        transform=data_transforms\n",
        "    )\n",
        "\n",
        "    # Extract class mappings\n",
        "    class_to_idx = full_dataset.class_to_idx  # Dict {class_name: index}\n",
        "    idx_to_class = {v: int(k) for k, v in class_to_idx.items()}  # Reverse mapping {index: class_name}\n",
        "\n",
        "    # Get total number of samples\n",
        "    dataset_size = len(full_dataset)\n",
        "\n",
        "    # Calculate validation size\n",
        "    val_size = int(dataset_size * test_split)\n",
        "    train_size = dataset_size - val_size\n",
        "\n",
        "    # Split dataset\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        full_dataset,\n",
        "        [train_size, val_size]\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=1\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, idx_to_class  # ✅ Now returning idx_to_class for predictions\n",
        "\n",
        "######################################################################################################################################################################\n",
        "##########################################################################################################################################################################\n",
        "##########################################################################################################################################################################\n",
        "\n",
        "# Optional: Data augmentation\n",
        "def get_augmented_transforms(image_size=(218, 178)):\n",
        "    \"\"\"\n",
        "    More comprehensive data augmentation\n",
        "    \"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "# Additional utility to count samples per class\n",
        "def count_samples_per_class(data_dir):\n",
        "    \"\"\"\n",
        "    Count number of samples in each class folder\n",
        "    \"\"\"\n",
        "    class_counts = {}\n",
        "    for class_folder in os.listdir(data_dir):\n",
        "        class_path = os.path.join(data_dir, class_folder)\n",
        "        if os.path.isdir(class_path):\n",
        "            num_samples = len(os.listdir(class_path))\n",
        "            class_counts[class_folder] = num_samples\n",
        "\n",
        "    return class_counts\n",
        "\n",
        "# Print class distribution\n",
        "def print_class_distribution(data_dir):\n",
        "    \"\"\"\n",
        "    Print number of samples in each class\n",
        "    \"\"\"\n",
        "    class_counts = count_samples_per_class(data_dir)\n",
        "    print(\"Class Distribution:\")\n",
        "    for cls, count in class_counts.items():\n",
        "        print(f\"Class {cls}: {count} samples\")"
      ],
      "metadata": {
        "id": "NYcANSliku1F"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/drive/MyDrive/CelebA_subset\"\n",
        "\n",
        "# Create data loaders\n",
        "train_loader, val_loader, class_names = create_data_loaders(data_dir)\n",
        "\n",
        "# Inspect first batch\n",
        "for images, labels in train_loader:\n",
        "    print(\"Batch images shape:\", images.shape)\n",
        "    print(\"Batch labels:\", labels)\n",
        "    print(\"Class names:\", class_names)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc1Ehoi8lBsG",
        "outputId": "81973774-1b24-45b6-b58e-37475e8f88e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch images shape: torch.Size([10, 3, 218, 178])\n",
            "Batch labels: tensor([1, 3, 6, 3, 8, 6, 3, 4, 8, 4])\n",
            "Class names: {0: 2820, 1: 3227, 2: 3699, 3: 3745, 4: 3782, 5: 4887, 6: 6568, 7: 8968, 8: 9152, 9: 9256}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNca9nJ1lHQ-",
        "outputId": "5b8c8ea0-37e5-47de-fa50-5705cb2b895d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training time !!\n",
        "![Image](https://media.tenor.com/oGNAlTqpMvYAAAAM/lets-do-this.gif)\n"
      ],
      "metadata": {
        "id": "NnJ-zHfonkQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# entities\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model1 = classfier()\n",
        "model1 = model1.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model1.parameters(), lr = 0.001)\"\"\""
      ],
      "metadata": {
        "id": "Eek87H0xl_Hy"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"num_epochs = 40\n",
        "\n",
        "for e in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for images, label in train_loader:\n",
        "        images, labels = images.to(device), label.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += label.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_acc = correct / total\n",
        "    print(f\"Epoch [{e+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "    # now evaluate on validation\n",
        "    model.eval()\n",
        "    val_correct, val_total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    print(f\"Validation Accuracy: {val_acc:.2f}%\\n\")\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vov0Q5Z6orHh",
        "outputId": "dbe4c302-f5d5-436b-e781-4184c8a832ea"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/40], Loss: 2.1085, Train Acc: 0.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 19.70%\n",
            "\n",
            "Epoch [2/40], Loss: 1.8003, Train Acc: 0.35%\n",
            "Validation Accuracy: 36.36%\n",
            "\n",
            "Epoch [3/40], Loss: 1.6385, Train Acc: 0.41%\n",
            "Validation Accuracy: 48.48%\n",
            "\n",
            "Epoch [4/40], Loss: 1.3287, Train Acc: 0.51%\n",
            "Validation Accuracy: 48.48%\n",
            "\n",
            "Epoch [5/40], Loss: 1.1550, Train Acc: 0.57%\n",
            "Validation Accuracy: 69.70%\n",
            "\n",
            "Epoch [6/40], Loss: 1.0539, Train Acc: 0.60%\n",
            "Validation Accuracy: 62.12%\n",
            "\n",
            "Epoch [7/40], Loss: 0.9733, Train Acc: 0.68%\n",
            "Validation Accuracy: 50.00%\n",
            "\n",
            "Epoch [8/40], Loss: 0.8377, Train Acc: 0.71%\n",
            "Validation Accuracy: 53.03%\n",
            "\n",
            "Epoch [9/40], Loss: 0.6395, Train Acc: 0.76%\n",
            "Validation Accuracy: 66.67%\n",
            "\n",
            "Epoch [10/40], Loss: 0.5709, Train Acc: 0.81%\n",
            "Validation Accuracy: 71.21%\n",
            "\n",
            "Epoch [11/40], Loss: 0.4039, Train Acc: 0.88%\n",
            "Validation Accuracy: 66.67%\n",
            "\n",
            "Epoch [12/40], Loss: 0.3088, Train Acc: 0.89%\n",
            "Validation Accuracy: 63.64%\n",
            "\n",
            "Epoch [13/40], Loss: 0.2324, Train Acc: 0.91%\n",
            "Validation Accuracy: 72.73%\n",
            "\n",
            "Epoch [14/40], Loss: 0.3141, Train Acc: 0.90%\n",
            "Validation Accuracy: 75.76%\n",
            "\n",
            "Epoch [15/40], Loss: 0.2157, Train Acc: 0.94%\n",
            "Validation Accuracy: 63.64%\n",
            "\n",
            "Epoch [16/40], Loss: 0.4089, Train Acc: 0.88%\n",
            "Validation Accuracy: 72.73%\n",
            "\n",
            "Epoch [17/40], Loss: 0.3478, Train Acc: 0.90%\n",
            "Validation Accuracy: 60.61%\n",
            "\n",
            "Epoch [18/40], Loss: 0.1937, Train Acc: 0.93%\n",
            "Validation Accuracy: 75.76%\n",
            "\n",
            "Epoch [19/40], Loss: 0.1618, Train Acc: 0.93%\n",
            "Validation Accuracy: 75.76%\n",
            "\n",
            "Epoch [20/40], Loss: 0.0637, Train Acc: 0.97%\n",
            "Validation Accuracy: 77.27%\n",
            "\n",
            "Epoch [21/40], Loss: 0.0906, Train Acc: 0.97%\n",
            "Validation Accuracy: 75.76%\n",
            "\n",
            "Epoch [22/40], Loss: 0.1879, Train Acc: 0.94%\n",
            "Validation Accuracy: 69.70%\n",
            "\n",
            "Epoch [23/40], Loss: 0.3061, Train Acc: 0.90%\n",
            "Validation Accuracy: 66.67%\n",
            "\n",
            "Epoch [24/40], Loss: 0.4577, Train Acc: 0.87%\n",
            "Validation Accuracy: 72.73%\n",
            "\n",
            "Epoch [25/40], Loss: 0.2265, Train Acc: 0.92%\n",
            "Validation Accuracy: 71.21%\n",
            "\n",
            "Epoch [26/40], Loss: 0.1772, Train Acc: 0.94%\n",
            "Validation Accuracy: 77.27%\n",
            "\n",
            "Epoch [27/40], Loss: 0.2320, Train Acc: 0.95%\n",
            "Validation Accuracy: 69.70%\n",
            "\n",
            "Epoch [28/40], Loss: 0.1733, Train Acc: 0.95%\n",
            "Validation Accuracy: 80.30%\n",
            "\n",
            "Epoch [29/40], Loss: 0.0338, Train Acc: 0.99%\n",
            "Validation Accuracy: 78.79%\n",
            "\n",
            "Epoch [30/40], Loss: 0.0192, Train Acc: 0.99%\n",
            "Validation Accuracy: 77.27%\n",
            "\n",
            "Epoch [31/40], Loss: 0.0277, Train Acc: 0.99%\n",
            "Validation Accuracy: 81.82%\n",
            "\n",
            "Epoch [32/40], Loss: 0.0144, Train Acc: 1.00%\n",
            "Validation Accuracy: 84.85%\n",
            "\n",
            "Epoch [33/40], Loss: 0.0233, Train Acc: 1.00%\n",
            "Validation Accuracy: 75.76%\n",
            "\n",
            "Epoch [34/40], Loss: 0.0180, Train Acc: 1.00%\n",
            "Validation Accuracy: 81.82%\n",
            "\n",
            "Epoch [35/40], Loss: 0.0897, Train Acc: 0.97%\n",
            "Validation Accuracy: 75.76%\n",
            "\n",
            "Epoch [36/40], Loss: 0.0794, Train Acc: 0.97%\n",
            "Validation Accuracy: 80.30%\n",
            "\n",
            "Epoch [37/40], Loss: 0.1186, Train Acc: 0.96%\n",
            "Validation Accuracy: 65.15%\n",
            "\n",
            "Epoch [38/40], Loss: 0.2269, Train Acc: 0.94%\n",
            "Validation Accuracy: 60.61%\n",
            "\n",
            "Epoch [39/40], Loss: 0.1793, Train Acc: 0.95%\n",
            "Validation Accuracy: 71.21%\n",
            "\n",
            "Epoch [40/40], Loss: 0.1086, Train Acc: 0.97%\n",
            "Validation Accuracy: 71.21%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model2\n",
        "model = FaceRecognizer()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 40\n",
        "best_val_acc = 0.0\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "for e in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_acc = (correct / total) * 100\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{e+1}/{num_epochs}], Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "    # now evaluate on validation\n",
        "    model.eval()\n",
        "    val_correct, val_total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    # update the best validation accuracy\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\\n\")\n",
        "\n",
        "    # learning rate scheduler\n",
        "    scheduler.step(val_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODq30y3srTCf",
        "outputId": "fd6fa226-bf88-4f3f-f8da-81f718d86f94"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/40], Loss: 1.6660, Train Acc: 48.30%\n",
            "Validation Accuracy: 89.39%\n",
            "Best Validation Accuracy: 89.39%\n",
            "\n",
            "Epoch [2/40], Loss: 0.6994, Train Acc: 82.26%\n",
            "Validation Accuracy: 90.91%\n",
            "Best Validation Accuracy: 90.91%\n",
            "\n",
            "Epoch [3/40], Loss: 0.4046, Train Acc: 90.57%\n",
            "Validation Accuracy: 92.42%\n",
            "Best Validation Accuracy: 92.42%\n",
            "\n",
            "Epoch [4/40], Loss: 0.3010, Train Acc: 93.96%\n",
            "Validation Accuracy: 93.94%\n",
            "Best Validation Accuracy: 93.94%\n",
            "\n",
            "Epoch [5/40], Loss: 0.2438, Train Acc: 95.47%\n",
            "Validation Accuracy: 93.94%\n",
            "Best Validation Accuracy: 93.94%\n",
            "\n",
            "Epoch [6/40], Loss: 0.2371, Train Acc: 94.34%\n",
            "Validation Accuracy: 89.39%\n",
            "Best Validation Accuracy: 93.94%\n",
            "\n",
            "Epoch [7/40], Loss: 0.1865, Train Acc: 96.23%\n",
            "Validation Accuracy: 86.36%\n",
            "Best Validation Accuracy: 93.94%\n",
            "\n",
            "Epoch [8/40], Loss: 0.2091, Train Acc: 95.09%\n",
            "Validation Accuracy: 95.45%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [9/40], Loss: 0.1775, Train Acc: 96.98%\n",
            "Validation Accuracy: 87.88%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [10/40], Loss: 0.1856, Train Acc: 96.23%\n",
            "Validation Accuracy: 90.91%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [11/40], Loss: 0.1210, Train Acc: 97.36%\n",
            "Validation Accuracy: 93.94%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [12/40], Loss: 0.1430, Train Acc: 97.36%\n",
            "Validation Accuracy: 89.39%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [13/40], Loss: 0.2214, Train Acc: 94.72%\n",
            "Validation Accuracy: 90.91%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [14/40], Loss: 0.2333, Train Acc: 93.96%\n",
            "Validation Accuracy: 92.42%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [15/40], Loss: 0.1659, Train Acc: 95.85%\n",
            "Validation Accuracy: 90.91%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [16/40], Loss: 0.0816, Train Acc: 98.87%\n",
            "Validation Accuracy: 89.39%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [17/40], Loss: 0.1456, Train Acc: 95.85%\n",
            "Validation Accuracy: 95.45%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [18/40], Loss: 0.0992, Train Acc: 98.11%\n",
            "Validation Accuracy: 95.45%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [19/40], Loss: 0.0817, Train Acc: 97.74%\n",
            "Validation Accuracy: 95.45%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [20/40], Loss: 0.0491, Train Acc: 99.62%\n",
            "Validation Accuracy: 92.42%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [21/40], Loss: 0.0579, Train Acc: 99.25%\n",
            "Validation Accuracy: 95.45%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [22/40], Loss: 0.0681, Train Acc: 99.25%\n",
            "Validation Accuracy: 92.42%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [23/40], Loss: 0.0840, Train Acc: 98.11%\n",
            "Validation Accuracy: 95.45%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [24/40], Loss: 0.0367, Train Acc: 100.00%\n",
            "Validation Accuracy: 95.45%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [25/40], Loss: 0.0348, Train Acc: 100.00%\n",
            "Validation Accuracy: 95.45%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [26/40], Loss: 0.0828, Train Acc: 98.11%\n",
            "Validation Accuracy: 95.45%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [27/40], Loss: 0.0300, Train Acc: 100.00%\n",
            "Validation Accuracy: 95.45%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [28/40], Loss: 0.0322, Train Acc: 99.62%\n",
            "Validation Accuracy: 95.45%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [29/40], Loss: 0.0218, Train Acc: 100.00%\n",
            "Validation Accuracy: 95.45%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [30/40], Loss: 0.0404, Train Acc: 99.25%\n",
            "Validation Accuracy: 95.45%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [31/40], Loss: 0.0366, Train Acc: 99.62%\n",
            "Validation Accuracy: 95.45%\n",
            "Best Validation Accuracy: 95.45%\n",
            "\n",
            "Epoch [32/40], Loss: 0.0262, Train Acc: 99.62%\n",
            "Validation Accuracy: 96.97%\n",
            "Best Validation Accuracy: 96.97%\n",
            "\n",
            "Epoch [33/40], Loss: 0.0377, Train Acc: 99.25%\n",
            "Validation Accuracy: 96.97%\n",
            "Best Validation Accuracy: 96.97%\n",
            "\n",
            "Epoch [34/40], Loss: 0.0443, Train Acc: 98.87%\n",
            "Validation Accuracy: 96.97%\n",
            "Best Validation Accuracy: 96.97%\n",
            "\n",
            "Epoch [35/40], Loss: 0.0220, Train Acc: 100.00%\n",
            "Validation Accuracy: 96.97%\n",
            "Best Validation Accuracy: 96.97%\n",
            "\n",
            "Epoch [36/40], Loss: 0.0394, Train Acc: 99.62%\n",
            "Validation Accuracy: 96.97%\n",
            "Best Validation Accuracy: 96.97%\n",
            "\n",
            "Epoch [37/40], Loss: 0.0173, Train Acc: 100.00%\n",
            "Validation Accuracy: 96.97%\n",
            "Best Validation Accuracy: 96.97%\n",
            "\n",
            "Epoch [38/40], Loss: 0.0177, Train Acc: 100.00%\n",
            "Validation Accuracy: 96.97%\n",
            "Best Validation Accuracy: 96.97%\n",
            "\n",
            "Epoch [39/40], Loss: 0.0274, Train Acc: 100.00%\n",
            "Validation Accuracy: 96.97%\n",
            "Best Validation Accuracy: 96.97%\n",
            "\n",
            "Epoch [40/40], Loss: 0.0248, Train Acc: 100.00%\n",
            "Validation Accuracy: 96.97%\n",
            "Best Validation Accuracy: 96.97%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### try that model on occluded face dataset\n",
        "### make a dataloader for that with labels"
      ],
      "metadata": {
        "id": "v6zYM-f0WyJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/drive/MyDrive/occluded_dataset_BIG\"\n",
        "dirs = os.listdir(data_dir)\n",
        "dirs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfVFQ0gAWmJA",
        "outputId": "134810b9-da5e-4da9-d266-b6fe2719841c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['8968',\n",
              " '6568',\n",
              " '2820',\n",
              " '3745',\n",
              " '3227',\n",
              " '3782',\n",
              " '3699',\n",
              " '9256',\n",
              " '4887',\n",
              " '9152']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7vDrQfR4Y1Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the same transformation as used in training\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((218, 178)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Define the dataset class (without labels for inference)\n",
        "class FaceDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.true_labels = []\n",
        "\n",
        "        self.class_names = sorted(os.listdir(root_dir))  # Get class names (numeric)\n",
        "        self.class_to_idx = {cls_name: int(cls_name) for cls_name in self.class_names}  # Keep numeric class names\n",
        "\n",
        "        for class_folder in self.class_names:\n",
        "            class_path = os.path.join(root_dir, class_folder)\n",
        "            if os.path.isdir(class_path):\n",
        "                for img_file in os.listdir(class_path):\n",
        "                    img_path = os.path.join(class_path, img_file)\n",
        "                    self.image_paths.append(img_path)\n",
        "                    self.true_labels.append(self.class_to_idx[class_folder])  # Store class label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        true_label = self.true_labels[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")  # Ensure it's RGB\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, true_label, img_path  # Return image, actual class (folder name), and path\n",
        "\n",
        "# Set dataset path and create DataLoader\n",
        "eval_dir = \"/content/drive/MyDrive/occluded_dataset_BIG\"  # Update this path\n",
        "dataset = FaceDataset(root_dir=eval_dir, transform=data_transforms)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "6ixzy7M-X0fi"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names_dic = {0: 2820, 1: 3227, 2: 3699, 3: 3745, 4: 3782, 5: 4887, 6: 6568, 7: 8968, 8: 9152, 9: 9256}"
      ],
      "metadata": {
        "id": "44Cl1cJGbXKN"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Evaluation\n",
        "correct, total = 0, 0\n",
        "class_names = dataset.class_names  # Numeric class names\n",
        "\n",
        "with torch.no_grad():\n",
        "    for img, true_label, img_path in dataloader:\n",
        "        img = img.to(device)\n",
        "        true_label = true_label.item()\n",
        "\n",
        "        # Get model prediction\n",
        "        output = model(img)\n",
        "        predicted_idx = output.argmax(dim=1).item()\n",
        "\n",
        "        # Check if prediction is correct\n",
        "        total += 1\n",
        "        predicted = class_names_dic[predicted_idx]\n",
        "\n",
        "        if predicted == true_label:\n",
        "            correct += 1\n",
        "        #print(predicted_idx, true_label)\n",
        "        #print(f\"Image: {os.path.basename(img_path[0])} → Predicted: {class_names_dic[predicted_idx]}, Actual: {true_label}\")\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = (correct / total) * 100\n",
        "print(f\"\\nModel Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbp1IRnCaXR6",
        "outputId": "486884bc-b8e0-4ec3-cd42-725b7e11a5ba"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Accuracy: 99.11%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/occluded_dataset_BIG/classifier_model.pth\")"
      ],
      "metadata": {
        "id": "RENReK4XbRBL"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for reuse of model follow this commands\n",
        "\"\"\"\n",
        "model = Classifier()  # Recreate the model structure\n",
        "model.load_state_dict(torch.load(\"model.pth\"))\n",
        "model.eval()  # Set to evaluation mode\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "z1_4B4lqis_-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}